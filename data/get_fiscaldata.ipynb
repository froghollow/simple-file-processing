{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Files from FiscalData\n",
    "\n",
    "This notebook demonstrates how to use Python and AWS SDK for Python (boto3) to download public information available via REST APIs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Set up Tables in Glue Catalog\n",
    "Create Glue Tables using Metadata from [FiscalData Data Dictionaries](data/metadata/readme.md).\n",
    "\n",
    "**Why not Glue Crawler?**   \n",
    "Alternatively, we can [use a crawler](https://docs.aws.amazon.com/glue/latest/dg/add-crawler.html) to populate the AWS Glue Data Catalog with tables. This is the primary method used by most AWS Glue users. A crawler can crawl multiple data stores in a single run. Upon completion, the crawler creates or updates one or more tables in your Data Catalog.\n",
    "\n",
    "Experience with crawlers has revealed them to be 'fiddly' to configure and maintain. And inconsistent inferring data types.  Since we generally have good knowledge of the data we are working with, including metadata and/or schemas from which to specify accurate data types, our preferred approach is to explicity create the Glue Tables.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'S_sys_abbrev' (str)\n",
      "Stored 'S_landing_pad_bucket' (str)\n",
      "Stored 'S_glue_database' (str)\n",
      "Stored 'S_datalake_bucket' (str)\n",
      "Stored variables and their in-db values:\n",
      "S_account_id                     -> '{aws_acct}'\n",
      "S_arn_template                   -> 'arn:aws:$Service:us-east-2:{aws_acct}:$Resource\n",
      "S_code_bucket                    -> 'daab-lab-code-us-east-2'\n",
      "S_datalake_bucket                -> 'daab-lab-smpl-main-datalake'\n",
      "S_file_prefix                    -> 'FDMD.FSDATA'\n",
      "S_glue_database                  -> 'daab-lab-smpl-main-fsdata'\n",
      "S_glue_dbname                    -> 'daab-lab-smpl-main-fsdata'\n",
      "S_landing_pad_bucket             -> 'daab-lab-smpl-main-landing-pad'\n",
      "S_partition                      -> 'aws'\n",
      "S_qualifier                      -> 'FDMD'\n",
      "S_region                         -> 'us-east-2'\n",
      "S_rootdir                        -> '/home/ec2-user/SageMaker/daab-simple'\n",
      "S_stack                          -> 'daab-lab-smpl-main'\n",
      "S_sys_abbrev                     -> 'FSDATA'\n"
     ]
    }
   ],
   "source": [
    "# Initialize Persistent %store & Notebook Globals\n",
    "import json\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "%store -r \n",
    "\n",
    "S_sys_abbrev = \"FSDATA\"\n",
    "S_landing_pad_bucket = f\"{S_stack}-landing-pad\"\n",
    "S_datalake_bucket = f\"{S_stack}-datalake\"\n",
    "\n",
    "S_glue_database = f\"{S_stack}-{S_sys_abbrev}\".lower()\n",
    "\n",
    "%store S_sys_abbrev S_landing_pad_bucket S_glue_database S_datalake_bucket\n",
    "\n",
    "%store \n",
    "\n",
    "os.environ['AWS_DEFAULT_REGION'] = S_region\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Existing Glue Database 'daab-lab-smpl-main-fsdata'\n"
     ]
    }
   ],
   "source": [
    "# create a Glue Database Using Python SDK\n",
    "# (shoulda already been created by the CloudFormation stack)\n",
    "glue_client = boto3.client('glue')\n",
    "\n",
    "try:\n",
    "    response = glue_client.create_database(\n",
    "        DatabaseInput={\n",
    "            'Name': S_glue_database,\n",
    "            'Description': 'Created from boto3 script in glue_workbook.ipynb',\n",
    "            'LocationUri': f's3://{S_stack}-datalake/{S_sys_abbrev}'\n",
    "        }\n",
    "    )\n",
    "    print(f\"Created Glue Database '{S_glue_database}'\")\n",
    "except glue_client.exceptions.AlreadyExistsException:\n",
    "    print(f\"Using Existing Glue Database '{S_glue_database}'\")\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# create Glue Tables Using Python SDK\n",
    "import boto3\n",
    "glue_client = boto3.client(\"glue\")\n",
    "\n",
    "import sys\n",
    "python_path = f\"{S_rootdir}/python\"\n",
    "if python_path not in sys.path: sys.path.append(python_path)\n",
    "import glue_functions_2212 as glu\n",
    "\n",
    "glue_table_names = [\n",
    "  'avg_interest_rates',\n",
    "  'top_federal',\n",
    "  'top_state'\n",
    "]\n",
    "\n",
    "glue_table_input = json.loads(glu.get_file('file://../python/glue_table_input_template_parquet.json'))\n",
    "\n",
    "for glue_table_name in glue_table_names:\n",
    "  \n",
    "  with open( f\"metadata/{glue_table_name}.raml\" , \"r\" ) as f:\n",
    "    raml = f.read()\n",
    "  column_metadata = glu.import_raml_to_glue( \"\", \"\", raml )\n",
    "  s3_url_table_location = f's3://{S_datalake_bucket}/{S_sys_abbrev}/PARQUET/{glue_table_name}'\n",
    "\n",
    "  glue_table_input['TableInput']['Name'] = glue_table_name\n",
    "  glue_table_input['TableInput']['StorageDescriptor']['Columns'] = column_metadata\n",
    "  glue_table_input['TableInput']['StorageDescriptor']['Location'] = s3_url_table_location\n",
    "\n",
    "  try:\n",
    "    response = glue_client.create_table(\n",
    "      DatabaseName = S_glue_database,\n",
    "      TableInput = glue_table_input['TableInput']  \n",
    "    )\n",
    "    status = 'Created'\n",
    "  except glue_client.exceptions.AlreadyExistsException:\n",
    "    response = glue_client.update_table (\n",
    "        DatabaseName = S_glue_database,\n",
    "        TableInput = glue_table_input['TableInput']\n",
    "    )\n",
    "    status = 'Updated'\n",
    "\n",
    "  print(f\"Glue Table '{glue_table_name}' {status} in Database '{S_glue_database}'\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fetch & Package Data \n",
    "Get some files from FiscalData via REST API \n",
    "-   [aws_setup.ipynb](aws_setup.ipynb)\n",
    "-   package into a ZIP archive file containing a set of CSV files (typical format for Bureau data exchanges)\n",
    "    -   {dataset}.{Dyymmdd}.full.csv\n",
    "-   This example program uploads the file to S3 using the AWS SDK for Python.   Alternatively, the ZIP file might be placed in the designated folder by a MFT process. \n",
    "- Process for obtaining data files from Enterprise Anypoint REST API will be similar, but will need to include authentication headers in requests.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'top_federal.D230113.full.csv' written to 'FDMD.FSDATA.D230113.FULL.ZIP'\n",
      "'top_state.D230113.full.csv' written to 'FDMD.FSDATA.D230113.FULL.ZIP'\n",
      "'avg_interest_rates.D230113.full.csv' written to 'FDMD.FSDATA.D230113.FULL.ZIP'\n"
     ]
    }
   ],
   "source": [
    "# Download Several FiscalData Data Sets, package into .ZIP\n",
    "from urllib.parse import urlparse\n",
    "import datetime\n",
    "import os\n",
    "import boto3\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "base_url = \"https://api.fiscaldata.treasury.gov/services/api/fiscal_service\"\n",
    "\n",
    "api_data_urls = [\n",
    "    f\"{base_url}/v1/debt/top/top_federal?page[number]=1&page[size]=100&format=csv&filter=record_date:eq:2022-10-01\",\n",
    "    f\"{base_url}/v1/debt/top/top_state?page[number]=1&page[size]=100&format=csv&filter=record_date:eq:2022-10-01\",\n",
    "    f\"{base_url}/v2/accounting/od/avg_interest_rates?sort=-record_date&format=csv&page[number]=1&page[size]=216\"\n",
    "]\n",
    "\n",
    "yymmdd = datetime.datetime.now().strftime(\"%y%m%d\")\n",
    "zipfile_name = f\"FDMD.{S_sys_abbrev}.D{yymmdd}.FULL.ZIP\"\n",
    "#cwd = os.getcwd()\n",
    "outfolder = f\"{S_rootdir}/data\"\n",
    "if not os.path.exists(outfolder):\n",
    "    os.makedirs(outfolder)\n",
    "\n",
    "for url in api_data_urls:\n",
    "    parts = urlparse(url)\n",
    "    #print(parts)\n",
    "    dataset_name = parts.path.split('/')[-1]\n",
    "    member_filename = f\"{dataset_name}.D{yymmdd}.full.csv\"\n",
    "\n",
    "    rows = requests.get(url)\n",
    "\n",
    "    with zipfile.ZipFile(f\"{outfolder}/{zipfile_name}\", 'a') as outzip:\n",
    "        outzip.writestr( f\"{member_filename}\" , rows.text )\n",
    "\n",
    "    print (f\"'{member_filename}' written to '{zipfile_name}'\")\n",
    "\n",
    "#s3_client.upload_file( f\"{outfolder}/{zipfile_name}\", s3_bucket, f'{s3_folder}/{zipfile_name}' )\n",
    "#s3_url_zipfile = f\"s3://{s3_bucket}/{s3_folder}/{zipfile_name}\"\n",
    "#print(f\"'{zipfile_name}' uploaded to 's3://{s3_bucket}/{s3_folder}/{zipfile_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'FDMD.FSDATA.D230113.FULL.ZIP' uploaded to 's3://daab-lab-smpl-main-landing-pad/FSDATA/Inbound/FDMD.FSDATA.D230113.FULL.ZIP'\n"
     ]
    }
   ],
   "source": [
    "# Uploading the ZIP file to the S3'landing-pad' bucket will initiate processing thru the EventBridge Rule ...\n",
    "s3_client = boto3.client('s3')\n",
    "s3_bucket = S_landing_pad_bucket\n",
    "s3_folder = f\"{S_sys_abbrev}/Inbound\"\n",
    "\n",
    "s3_client.upload_file( f\"{outfolder}/{zipfile_name}\", s3_bucket, f'{s3_folder}/{zipfile_name}' )\n",
    "s3_url_zipfile = f\"s3://{s3_bucket}/{s3_folder}/{zipfile_name}\"\n",
    "print(f\"'{zipfile_name}' uploaded to 's3://{s3_bucket}/{s3_folder}/{zipfile_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "9650cb4e16cdd4a8e8e2d128bf38d875813998db22a3c986335f89e0cb4d7bb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
